{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Downloading https:\\github.com\\ultralytics\\assets\\releases\\download\\v0.0.0\\yolov8n.pt to yolov8n.pt...\n",
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 11.4MB/s]\n",
      "New https://pypi.org/project/ultralytics/8.0.82 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "\n",
      "Dataset 'coco128.yaml' images not found , missing paths ['C:\\\\Users\\\\dani\\\\Python\\\\34759_Perception\\\\datasets\\\\coco128\\\\images\\\\train2017']\n",
      "Downloading https:\\ultralytics.com\\assets\\coco128.zip to C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128.zip...\n",
      "100%|██████████| 6.66M/6.66M [00:00<00:00, 10.2MB/s]\n",
      "Unzipping C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128.zip to C:\\Users\\dani\\Python\\34759_Perception\\datasets...\n",
      "Dataset download success  (1.8s), saved to \u001b[1mC:\\Users\\dani\\Python\\34759_Perception\\datasets\u001b[0m\n",
      "\n",
      "Downloading https:\\ultralytics.com\\assets\\Arial.ttf to C:\\Users\\dani\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n",
      "100%|██████████| 755k/755k [00:00<00:00, 8.72MB/s]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<00:00, 606.26it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G      1.169        1.4      1.202        257        640: 100%|██████████| 8/8 [00:42<00:00,  5.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]\n",
      "                   all        128        929      0.656      0.564      0.622       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G       1.19      1.406      1.245        174        640: 100%|██████████| 8/8 [00:41<00:00,  5.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:15<00:00,  3.91s/it]\n",
      "                   all        128        929      0.655      0.583      0.647      0.478\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G      1.148      1.314      1.241        223        640: 100%|██████████| 8/8 [00:42<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]\n",
      "                   all        128        929      0.676       0.59      0.664      0.491\n",
      "\n",
      "3 epochs completed in 0.049 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]\n",
      "                   all        128        929      0.692       0.58      0.665      0.491\n",
      "                person        128        254      0.798      0.667      0.768       0.55\n",
      "               bicycle        128          6      0.925      0.333      0.376      0.313\n",
      "                   car        128         46      0.909      0.217      0.315      0.193\n",
      "            motorcycle        128          5      0.698        0.8      0.895      0.732\n",
      "              airplane        128          6      0.728      0.833      0.955      0.742\n",
      "                   bus        128          7      0.753      0.714      0.721      0.654\n",
      "                 train        128          3      0.674          1      0.913      0.797\n",
      "                 truck        128         12      0.921        0.5       0.53      0.358\n",
      "                  boat        128          6      0.307      0.167      0.392       0.28\n",
      "         traffic light        128         14      0.658      0.214      0.227      0.143\n",
      "             stop sign        128          2      0.781          1      0.995      0.718\n",
      "                 bench        128          9      0.806      0.556      0.622      0.485\n",
      "                  bird        128         16      0.839      0.875       0.95      0.616\n",
      "                   cat        128          4      0.714          1      0.895      0.684\n",
      "                   dog        128          9      0.557      0.778       0.79      0.548\n",
      "                 horse        128          2      0.693          1      0.995      0.484\n",
      "              elephant        128         17      0.758      0.941      0.939      0.748\n",
      "                  bear        128          1       0.49          1      0.995      0.995\n",
      "                 zebra        128          4      0.866          1      0.995      0.966\n",
      "               giraffe        128          9      0.681          1      0.973      0.718\n",
      "              backpack        128          6      0.382      0.215      0.353      0.198\n",
      "              umbrella        128         18      0.622      0.556      0.678      0.463\n",
      "               handbag        128         19          1      0.117       0.29      0.162\n",
      "                   tie        128          7      0.787      0.714      0.704      0.491\n",
      "              suitcase        128          4       0.64      0.899      0.849      0.568\n",
      "               frisbee        128          5      0.742        0.8      0.759      0.655\n",
      "                  skis        128          1       0.81          1      0.995      0.522\n",
      "             snowboard        128          7      0.726      0.857      0.865      0.518\n",
      "           sports ball        128          6      0.715       0.43      0.556      0.276\n",
      "                  kite        128         10      0.667        0.5      0.561      0.189\n",
      "          baseball bat        128          4      0.412      0.368      0.374      0.211\n",
      "        baseball glove        128          7      0.752      0.429       0.43      0.303\n",
      "            skateboard        128          5      0.759        0.6        0.6      0.407\n",
      "         tennis racket        128          7      0.519      0.313      0.545      0.326\n",
      "                bottle        128         18      0.475      0.444      0.416      0.255\n",
      "            wine glass        128         16      0.581      0.562      0.643      0.387\n",
      "                   cup        128         36      0.687      0.305      0.422      0.297\n",
      "                  fork        128          6      0.641      0.167        0.2      0.173\n",
      "                 knife        128         16      0.674      0.517      0.652      0.389\n",
      "                 spoon        128         22      0.819      0.207      0.354      0.198\n",
      "                  bowl        128         28      0.644      0.714      0.675      0.558\n",
      "                banana        128          1          0          0      0.199     0.0796\n",
      "              sandwich        128          2      0.959          1      0.995      0.995\n",
      "                orange        128          4          1          0      0.828      0.531\n",
      "              broccoli        128         11      0.586       0.26      0.293      0.238\n",
      "                carrot        128         24      0.674      0.542      0.732       0.48\n",
      "               hot dog        128          2      0.621          1      0.828      0.763\n",
      "                 pizza        128          5      0.776          1      0.995      0.841\n",
      "                 donut        128         14      0.633          1      0.952      0.865\n",
      "                  cake        128          4      0.879          1      0.995       0.89\n",
      "                 chair        128         35      0.569       0.49      0.481      0.297\n",
      "                 couch        128          6      0.557      0.429      0.708      0.517\n",
      "          potted plant        128         14      0.692      0.643      0.708      0.487\n",
      "                   bed        128          3       0.88          1      0.995      0.771\n",
      "          dining table        128         13      0.519      0.615      0.518      0.422\n",
      "                toilet        128          2          1      0.899      0.995      0.896\n",
      "                    tv        128          2       0.57        0.5      0.828      0.762\n",
      "                laptop        128          3          1          0      0.544       0.46\n",
      "                 mouse        128          2          1          0     0.0523    0.00523\n",
      "                remote        128          8      0.837        0.5      0.573      0.479\n",
      "            cell phone        128          8          0          0     0.0935     0.0471\n",
      "             microwave        128          3      0.512      0.706      0.753      0.654\n",
      "                  oven        128          5      0.457        0.4      0.485      0.353\n",
      "                  sink        128          6      0.371      0.167      0.495      0.255\n",
      "          refrigerator        128          5      0.697        0.6      0.743      0.616\n",
      "                  book        128         29      0.504       0.14      0.381       0.22\n",
      "                 clock        128          9       0.89      0.778      0.901      0.742\n",
      "                  vase        128          2       0.51          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.497      0.159\n",
      "            teddy bear        128         21      0.865       0.61      0.726      0.476\n",
      "            toothbrush        128          5          1      0.578      0.906      0.559\n",
      "Speed: 1.2ms preprocess, 74.2ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:10<00:00,  1.37s/it]\n",
      "                   all        128        929      0.728       0.58      0.668      0.492\n",
      "                person        128        254      0.799      0.657      0.768      0.551\n",
      "               bicycle        128          6      0.959      0.333      0.374      0.311\n",
      "                   car        128         46      0.912      0.217      0.316      0.193\n",
      "            motorcycle        128          5      0.661        0.8      0.895      0.725\n",
      "              airplane        128          6      0.733      0.833      0.955      0.742\n",
      "                   bus        128          7      0.769      0.714      0.721      0.654\n",
      "                 train        128          3      0.677          1      0.913      0.797\n",
      "                 truck        128         12       0.93        0.5       0.53      0.349\n",
      "                  boat        128          6       0.34      0.167      0.398      0.262\n",
      "         traffic light        128         14      0.674      0.214      0.209      0.141\n",
      "             stop sign        128          2      0.788          1      0.995      0.721\n",
      "                 bench        128          9      0.815      0.556      0.622      0.485\n",
      "                  bird        128         16      0.846      0.875      0.947      0.613\n",
      "                   cat        128          4      0.689          1      0.895      0.684\n",
      "                   dog        128          9      0.563      0.778       0.79      0.548\n",
      "                 horse        128          2        0.7          1      0.995      0.487\n",
      "              elephant        128         17       0.76      0.941       0.94      0.748\n",
      "                  bear        128          1      0.494          1      0.995      0.995\n",
      "                 zebra        128          4      0.868          1      0.995      0.959\n",
      "               giraffe        128          9      0.745          1      0.951      0.722\n",
      "              backpack        128          6      0.361      0.194      0.352      0.214\n",
      "              umbrella        128         18      0.628      0.556      0.676      0.462\n",
      "               handbag        128         19          1      0.112      0.285      0.159\n",
      "                   tie        128          7      0.791      0.714      0.703       0.49\n",
      "              suitcase        128          4      0.622      0.833      0.849      0.568\n",
      "               frisbee        128          5      0.748        0.8      0.759      0.655\n",
      "                  skis        128          1      0.822          1      0.995      0.522\n",
      "             snowboard        128          7      0.737      0.857      0.864      0.518\n",
      "           sports ball        128          6      0.712      0.424      0.556      0.276\n",
      "                  kite        128         10      0.684        0.5      0.562       0.19\n",
      "          baseball bat        128          4      0.338       0.25      0.349      0.199\n",
      "        baseball glove        128          7      0.689      0.429       0.43      0.317\n",
      "            skateboard        128          5      0.824        0.6        0.6       0.42\n",
      "         tennis racket        128          7      0.717      0.373       0.59       0.36\n",
      "                bottle        128         18      0.447      0.406      0.425      0.267\n",
      "            wine glass        128         16      0.611      0.562      0.641      0.384\n",
      "                   cup        128         36      0.666      0.333      0.443      0.307\n",
      "                  fork        128          6      0.645      0.167      0.207      0.178\n",
      "                 knife        128         16      0.581       0.52      0.625      0.371\n",
      "                 spoon        128         22        0.8      0.227      0.365      0.203\n",
      "                  bowl        128         28      0.679       0.75      0.704      0.558\n",
      "                banana        128          1          1          0      0.166     0.0746\n",
      "              sandwich        128          2          1      0.849      0.995      0.995\n",
      "                orange        128          4          1          0      0.828      0.531\n",
      "              broccoli        128         11      0.419      0.182      0.283      0.227\n",
      "                carrot        128         24      0.713      0.621       0.74      0.477\n",
      "               hot dog        128          2      0.628          1      0.828      0.796\n",
      "                 pizza        128          5      0.803          1      0.995      0.844\n",
      "                 donut        128         14      0.635          1      0.946      0.855\n",
      "                  cake        128          4      0.794          1      0.995       0.89\n",
      "                 chair        128         35      0.555      0.457      0.465      0.289\n",
      "                 couch        128          6      0.795      0.651      0.829       0.62\n",
      "          potted plant        128         14      0.698      0.643      0.708      0.487\n",
      "                   bed        128          3      0.892          1      0.995      0.679\n",
      "          dining table        128         13      0.617      0.615      0.533      0.411\n",
      "                toilet        128          2          1      0.893      0.995      0.896\n",
      "                    tv        128          2      0.575        0.5      0.828      0.763\n",
      "                laptop        128          3          1          0      0.552      0.471\n",
      "                 mouse        128          2          1          0     0.0776     0.0155\n",
      "                remote        128          8      0.842        0.5      0.578      0.483\n",
      "            cell phone        128          8          1          0     0.0935     0.0469\n",
      "             microwave        128          3      0.505      0.684      0.863      0.734\n",
      "                  oven        128          5      0.434        0.4      0.485      0.353\n",
      "                  sink        128          6      0.374      0.167      0.456      0.243\n",
      "          refrigerator        128          5      0.722        0.6      0.743      0.599\n",
      "                  book        128         29      0.571      0.138      0.396      0.228\n",
      "                 clock        128          9      0.893      0.778        0.9      0.742\n",
      "                  vase        128          2      0.514          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.497      0.159\n",
      "            teddy bear        128         21       0.86      0.588      0.722      0.472\n",
      "            toothbrush        128          5          1       0.71      0.906      0.525\n",
      "Speed: 1.2ms preprocess, 72.8ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "\n",
      "Downloading https:\\ultralytics.com\\images\\bus.jpg to bus.jpg...\n",
      "100%|██████████| 476k/476k [00:00<00:00, 970kB/s]\n",
      "image 1/1 C:\\Users\\dani\\Python\\34759_Perception\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 88.4ms\n",
      "Speed: 0.0ms preprocess, 88.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs\\detect\\train\\weights\\best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv8 requirement \"onnx>=1.12.0\" not found, attempting AutoUpdate...\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.13.1-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 8.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\dani\\anaconda3\\envs\\project\\lib\\site-packages (from onnx>=1.12.0) (4.5.0)\n",
      "Collecting protobuf<4,>=3.20.2\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "     ------------------------------------- 904.0/904.0 kB 14.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\dani\\anaconda3\\envs\\project\\lib\\site-packages (from onnx>=1.12.0) (1.24.2)\n",
      "Installing collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.13.1 protobuf-3.20.3\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  14.0s, saved as runs\\detect\\train\\weights\\best.onnx (12.2 MB)\n",
      "\n",
      "Export complete (14.2s)\n",
      "Results saved to \u001b[1mC:\\Users\\dani\\Python\\34759_Perception\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 data=C:\\Users\\dani\\anaconda3\\envs\\project\\Lib\\site-packages\\ultralytics\\datasets\\coco128.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new Yolo model\n",
    "model = YOLO('yolov8n.yaml')\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train the model using the 'coco128.yaml' dataset for 3 epochs\n",
    "results = model.train(data='coco128.yaml', epochs=3)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "results = model.val()\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model('https://ultralytics.com/images/bus.jpg')\n",
    "\n",
    "# Export the model to ONNX format\n",
    "success = model.export(format='onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:11<00:00,  1.39s/it]\n",
      "                   all        128        929      0.728       0.58      0.668      0.492\n",
      "                person        128        254      0.799      0.657      0.768      0.551\n",
      "               bicycle        128          6      0.959      0.333      0.374      0.311\n",
      "                   car        128         46      0.912      0.217      0.316      0.193\n",
      "            motorcycle        128          5      0.661        0.8      0.895      0.725\n",
      "              airplane        128          6      0.733      0.833      0.955      0.742\n",
      "                   bus        128          7      0.769      0.714      0.721      0.654\n",
      "                 train        128          3      0.677          1      0.913      0.797\n",
      "                 truck        128         12       0.93        0.5       0.53      0.349\n",
      "                  boat        128          6       0.34      0.167      0.398      0.262\n",
      "         traffic light        128         14      0.674      0.214      0.209      0.141\n",
      "             stop sign        128          2      0.788          1      0.995      0.721\n",
      "                 bench        128          9      0.815      0.556      0.622      0.485\n",
      "                  bird        128         16      0.846      0.875      0.947      0.613\n",
      "                   cat        128          4      0.689          1      0.895      0.684\n",
      "                   dog        128          9      0.563      0.778       0.79      0.548\n",
      "                 horse        128          2        0.7          1      0.995      0.487\n",
      "              elephant        128         17       0.76      0.941       0.94      0.748\n",
      "                  bear        128          1      0.494          1      0.995      0.995\n",
      "                 zebra        128          4      0.868          1      0.995      0.959\n",
      "               giraffe        128          9      0.745          1      0.951      0.722\n",
      "              backpack        128          6      0.361      0.194      0.352      0.214\n",
      "              umbrella        128         18      0.628      0.556      0.676      0.462\n",
      "               handbag        128         19          1      0.112      0.285      0.159\n",
      "                   tie        128          7      0.791      0.714      0.703       0.49\n",
      "              suitcase        128          4      0.622      0.833      0.849      0.568\n",
      "               frisbee        128          5      0.748        0.8      0.759      0.655\n",
      "                  skis        128          1      0.822          1      0.995      0.522\n",
      "             snowboard        128          7      0.737      0.857      0.864      0.518\n",
      "           sports ball        128          6      0.712      0.424      0.556      0.276\n",
      "                  kite        128         10      0.684        0.5      0.562       0.19\n",
      "          baseball bat        128          4      0.338       0.25      0.349      0.199\n",
      "        baseball glove        128          7      0.689      0.429       0.43      0.317\n",
      "            skateboard        128          5      0.824        0.6        0.6       0.42\n",
      "         tennis racket        128          7      0.717      0.373       0.59       0.36\n",
      "                bottle        128         18      0.447      0.406      0.425      0.267\n",
      "            wine glass        128         16      0.611      0.562      0.641      0.384\n",
      "                   cup        128         36      0.666      0.333      0.443      0.307\n",
      "                  fork        128          6      0.645      0.167      0.207      0.178\n",
      "                 knife        128         16      0.581       0.52      0.625      0.371\n",
      "                 spoon        128         22        0.8      0.227      0.365      0.203\n",
      "                  bowl        128         28      0.679       0.75      0.704      0.558\n",
      "                banana        128          1          1          0      0.166     0.0746\n",
      "              sandwich        128          2          1      0.849      0.995      0.995\n",
      "                orange        128          4          1          0      0.828      0.531\n",
      "              broccoli        128         11      0.419      0.182      0.283      0.227\n",
      "                carrot        128         24      0.713      0.621       0.74      0.477\n",
      "               hot dog        128          2      0.628          1      0.828      0.796\n",
      "                 pizza        128          5      0.803          1      0.995      0.844\n",
      "                 donut        128         14      0.635          1      0.946      0.855\n",
      "                  cake        128          4      0.794          1      0.995       0.89\n",
      "                 chair        128         35      0.555      0.457      0.465      0.289\n",
      "                 couch        128          6      0.795      0.651      0.829       0.62\n",
      "          potted plant        128         14      0.698      0.643      0.708      0.487\n",
      "                   bed        128          3      0.892          1      0.995      0.679\n",
      "          dining table        128         13      0.617      0.615      0.533      0.411\n",
      "                toilet        128          2          1      0.893      0.995      0.896\n",
      "                    tv        128          2      0.575        0.5      0.828      0.763\n",
      "                laptop        128          3          1          0      0.552      0.471\n",
      "                 mouse        128          2          1          0     0.0776     0.0155\n",
      "                remote        128          8      0.842        0.5      0.578      0.483\n",
      "            cell phone        128          8          1          0     0.0935     0.0469\n",
      "             microwave        128          3      0.505      0.684      0.863      0.734\n",
      "                  oven        128          5      0.434        0.4      0.485      0.353\n",
      "                  sink        128          6      0.374      0.167      0.456      0.243\n",
      "          refrigerator        128          5      0.722        0.6      0.743      0.599\n",
      "                  book        128         29      0.571      0.138      0.396      0.228\n",
      "                 clock        128          9      0.893      0.778        0.9      0.742\n",
      "                  vase        128          2      0.514          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.497      0.159\n",
      "            teddy bear        128         21       0.86      0.588      0.722      0.472\n",
      "            toothbrush        128          5          1       0.71      0.906      0.525\n",
      "Speed: 1.2ms preprocess, 73.1ms inference, 0.0ms loss, 2.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x0000022E5A5EB220>\n",
       "fitness: 0.5098954713297049\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.55107,     0.31111,     0.19345,     0.72492,     0.74191,     0.65397,     0.79698,     0.34852,     0.26208,     0.14073,     0.49235,       0.721,     0.49235,     0.48523,       0.613,     0.68431,     0.54845,      0.4874,     0.49235,     0.49235,     0.74843,       0.995,     0.95927,     0.72208,\n",
       "           0.21376,      0.4622,     0.15944,     0.48992,     0.56765,     0.65471,     0.52238,     0.51831,     0.27599,     0.19048,      0.1986,     0.31741,     0.41999,     0.49235,     0.36008,     0.26664,     0.38398,     0.30703,      0.1784,     0.37081,     0.20297,      0.5577,    0.074625,     0.49235,\n",
       "             0.995,     0.53075,     0.22706,     0.47694,     0.79575,     0.84392,     0.85543,     0.89015,     0.28877,     0.61966,     0.48717,     0.67911,     0.41097,     0.89619,     0.76255,     0.47128,    0.015521,     0.48266,     0.49235,     0.04686,     0.73429,     0.35298,     0.49235,     0.24349,\n",
       "           0.59878,      0.2276,     0.74157,      0.7455,     0.15858,     0.47193,     0.49235,     0.52462])\n",
       "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.7277399162948969, 'metrics/recall(B)': 0.5798276785161741, 'metrics/mAP50(B)': 0.667779534294416, 'metrics/mAP50-95(B)': 0.4923527976669592, 'fitness': 0.5098954713297049}\n",
       "save_dir: WindowsPath('runs/detect/val2')\n",
       "speed: {'preprocess': 1.2274626642465591, 'inference': 73.06399755179882, 'loss': 0.0, 'postprocess': 2.8379615396261215}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dani\\Python\\34759_Perception\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 115.7ms\n",
      "Speed: 1.0ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\n",
    "# Display preds. Accepts all YOLO predict arguments, save plotted images, dont save labels as labels\n",
    "results = model.predict(source=\"bus.jpg\", show=True, save=True, save_txt=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.90  Python-3.9.16 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/train/weights/best.pt, data=kitty_yolov8.yaml, epochs=3, patience=50, batch=16, imgsz=(1224, 370), save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train2\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.Detect                [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "WARNING  updating to 'imgsz=1224'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "WARNING  imgsz=[1224] must be multiple of max stride 32, updating to [1248]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\training\\labels.cache... 7481 images, 0 backgrounds, 0 corrupt: 100%|██████████| 7481/7481 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\val\\labels.cache... 354 images, 0 backgrounds, 0 corrupt: 100%|██████████| 354/354 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train2\\labels.jpg... \n",
      "Image sizes 1248 train, 1248 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train2\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3      7.66G      1.386      2.286      1.215        136       1248:  31%|███       | 145/468 [00:52<01:56,  2.78it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 290.00 MiB (GPU 0; 8.00 GiB total capacity; 6.59 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39mruns/detect/train/weights/best.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkitty_yolov8.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, imgsz\u001b[39m=\u001b[39;49m(\u001b[39m1224\u001b[39;49m, \u001b[39m370\u001b[39;49m), epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:370\u001b[0m, in \u001b[0;36mYOLO.train\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    371\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:191\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:324\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    322\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m    323\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(batch[\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 324\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(preds, batch)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m world_size\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\v8\\detect\\train.py:99\u001b[0m, in \u001b[0;36mDetectionTrainer.criterion\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcompute_loss\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss \u001b[39m=\u001b[39m Loss(de_parallel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel))\n\u001b[1;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(preds, batch)\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\v8\\detect\\train.py:211\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39m# pboxes\u001b[39;00m\n\u001b[0;32m    209\u001b[0m pred_bboxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[39m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m _, target_bboxes, target_scores, fg_mask, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massigner(\n\u001b[0;32m    212\u001b[0m     pred_scores\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49msigmoid(), (pred_bboxes\u001b[39m.\u001b[39;49mdetach() \u001b[39m*\u001b[39;49m stride_tensor)\u001b[39m.\u001b[39;49mtype(gt_bboxes\u001b[39m.\u001b[39;49mdtype),\n\u001b[0;32m    213\u001b[0m     anchor_points \u001b[39m*\u001b[39;49m stride_tensor, gt_labels, gt_bboxes, mask_gt)\n\u001b[0;32m    215\u001b[0m target_bboxes \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m stride_tensor\n\u001b[0;32m    216\u001b[0m target_scores_sum \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(target_scores\u001b[39m.\u001b[39msum(), \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\utils\\tal.py:111\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.forward\u001b[1;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[0;32m    106\u001b[0m     device \u001b[39m=\u001b[39m gt_bboxes\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39mfull_like(pd_scores[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbg_idx)\u001b[39m.\u001b[39mto(device), torch\u001b[39m.\u001b[39mzeros_like(pd_bboxes)\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m    108\u001b[0m             torch\u001b[39m.\u001b[39mzeros_like(pd_scores)\u001b[39m.\u001b[39mto(device), torch\u001b[39m.\u001b[39mzeros_like(pd_scores[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m    109\u001b[0m             torch\u001b[39m.\u001b[39mzeros_like(pd_scores[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m--> 111\u001b[0m mask_pos, align_metric, overlaps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_pos_mask(pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points,\n\u001b[0;32m    112\u001b[0m                                                      mask_gt)\n\u001b[0;32m    114\u001b[0m target_gt_idx, fg_mask, mask_pos \u001b[39m=\u001b[39m select_highest_overlaps(mask_pos, overlaps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_max_boxes)\n\u001b[0;32m    116\u001b[0m \u001b[39m# Assigned target\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\utils\\tal.py:130\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_pos_mask\u001b[1;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_pos_mask\u001b[39m(\u001b[39mself\u001b[39m, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n\u001b[0;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get in_gts mask, (b, max_num_obj, h*w).\"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     mask_in_gts \u001b[39m=\u001b[39m select_candidates_in_gts(anc_points, gt_bboxes)\n\u001b[0;32m    131\u001b[0m     \u001b[39m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     align_metric, overlaps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts \u001b[39m*\u001b[39m mask_gt)\n",
      "File \u001b[1;32mc:\\Users\\dani\\anaconda3\\envs\\tensor\\lib\\site-packages\\ultralytics\\yolo\\utils\\tal.py:25\u001b[0m, in \u001b[0;36mselect_candidates_in_gts\u001b[1;34m(xy_centers, gt_bboxes, eps)\u001b[0m\n\u001b[0;32m     23\u001b[0m bs, n_boxes, _ \u001b[39m=\u001b[39m gt_bboxes\u001b[39m.\u001b[39mshape\n\u001b[0;32m     24\u001b[0m lt, rb \u001b[39m=\u001b[39m gt_bboxes\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# left-top, right-bottom\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m bbox_deltas \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((xy_centers[\u001b[39mNone\u001b[39;49;00m] \u001b[39m-\u001b[39;49m lt, rb \u001b[39m-\u001b[39;49m xy_centers[\u001b[39mNone\u001b[39;49;00m]), dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mview(bs, n_boxes, n_anchors, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[39m# return (bbox_deltas.min(3)[0] > eps).to(gt_bboxes.dtype)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m bbox_deltas\u001b[39m.\u001b[39mamin(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mgt_(eps)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 0; 8.00 GiB total capacity; 6.59 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "\n",
    "results = model.train(data='kitty_yolov8.yaml', imgsz=(1224, 370), epochs=3, batch=8, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\val\\labels.cache... 354 images, 0 backgrounds, 0 corrupt: 100%|██████████| 354/354 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:41<00:00,  1.82s/it]\n",
      "                   all        354       4018    0.00566      0.174     0.0146    0.00494\n",
      "            Pedestrian        354       2809     0.0119       0.28     0.0294     0.0116\n",
      "               Cyclist        354        373          0          0          0          0\n",
      "                   Car        354        836    0.00505      0.243     0.0144    0.00322\n",
      "Speed: 1.7ms preprocess, 99.0ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x000001B1CC820550>\n",
       "fitness: 0.005903947397397156\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.01159,           0,   0.0032241])\n",
       "names: {0: 'Pedestrian', 1: 'Cyclist', 2: 'Car'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.005658272708688934, 'metrics/recall(B)': 0.1743312819412199, 'metrics/mAP50(B)': 0.014596204636409612, 'metrics/mAP50-95(B)': 0.004938141037506882, 'fitness': 0.005903947397397156}\n",
       "save_dir: WindowsPath('runs/detect/val2')\n",
       "speed: {'preprocess': 1.6500033901236153, 'inference': 99.04738940761587, 'loss': 0.0, 'postprocess': 3.181225162441448}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\testing\\images\\0000000000.png: 384x1248 (no detections), 170.0ms\n",
      "Speed: 8.0ms preprocess, 170.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1248, 1248)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source=\"./datasets/custom/testing/images/0000000000.png\", show=True, save=True, save_txt=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\testing\\images\\0000000000.png: 224x640 4 persons, 4 cars, 3 traffic lights, 70.1ms\n",
      "Speed: 0.0ms preprocess, 70.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[19, 28, 34],\n",
       "         [19, 30, 35],\n",
       "         [19, 31, 28],\n",
       "         ...,\n",
       "         [50, 51, 39],\n",
       "         [54, 58, 50],\n",
       "         [70, 52, 54]],\n",
       " \n",
       "        [[15, 28, 34],\n",
       "         [18, 30, 38],\n",
       "         [20, 30, 33],\n",
       "         ...,\n",
       "         [39, 74, 58],\n",
       "         [39, 65, 79],\n",
       "         [40, 46, 74]],\n",
       " \n",
       "        [[14, 24, 20],\n",
       "         [17, 27, 22],\n",
       "         [21, 28, 22],\n",
       "         ...,\n",
       "         [25, 50, 44],\n",
       "         [25, 41, 42],\n",
       "         [25, 37, 36]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[67, 51, 36],\n",
       "         [65, 50, 36],\n",
       "         [65, 51, 35],\n",
       "         ...,\n",
       "         [58, 54, 91],\n",
       "         [58, 58, 72],\n",
       "         [57, 54, 73]],\n",
       " \n",
       "        [[66, 51, 37],\n",
       "         [64, 51, 36],\n",
       "         [64, 51, 33],\n",
       "         ...,\n",
       "         [58, 61, 64],\n",
       "         [58, 63, 64],\n",
       "         [58, 64, 64]],\n",
       " \n",
       "        [[61, 52, 37],\n",
       "         [62, 52, 36],\n",
       "         [61, 52, 36],\n",
       "         ...,\n",
       "         [65, 65, 66],\n",
       "         [64, 62, 65],\n",
       "         [62, 62, 63]]], dtype=uint8)\n",
       " orig_shape: (370, 1224)\n",
       " path: 'C:\\\\Users\\\\dani\\\\Python\\\\34759_Perception\\\\datasets\\\\custom\\\\testing\\\\images\\\\0000000000.png'\n",
       " probs: None\n",
       " speed: {'preprocess': 0.0, 'inference': 70.12081146240234, 'postprocess': 0.0}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "model.predict(source=\"./datasets/custom/testing/images/0000000000.png\", show=True, save=True, save_txt=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4be9c908ebe39e7168c83d12fecab890f134a2bf8af6dfbceaf97ba05100539d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
