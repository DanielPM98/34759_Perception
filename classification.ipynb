{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning detection basic model on coco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Downloading https:\\github.com\\ultralytics\\assets\\releases\\download\\v0.0.0\\yolov8n.pt to yolov8n.pt...\n",
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 11.4MB/s]\n",
      "New https://pypi.org/project/ultralytics/8.0.82 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "\n",
      "Dataset 'coco128.yaml' images not found , missing paths ['C:\\\\Users\\\\dani\\\\Python\\\\34759_Perception\\\\datasets\\\\coco128\\\\images\\\\train2017']\n",
      "Downloading https:\\ultralytics.com\\assets\\coco128.zip to C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128.zip...\n",
      "100%|██████████| 6.66M/6.66M [00:00<00:00, 10.2MB/s]\n",
      "Unzipping C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128.zip to C:\\Users\\dani\\Python\\34759_Perception\\datasets...\n",
      "Dataset download success  (1.8s), saved to \u001b[1mC:\\Users\\dani\\Python\\34759_Perception\\datasets\u001b[0m\n",
      "\n",
      "Downloading https:\\ultralytics.com\\assets\\Arial.ttf to C:\\Users\\dani\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n",
      "100%|██████████| 755k/755k [00:00<00:00, 8.72MB/s]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<00:00, 606.26it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G      1.169        1.4      1.202        257        640: 100%|██████████| 8/8 [00:42<00:00,  5.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]\n",
      "                   all        128        929      0.656      0.564      0.622       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G       1.19      1.406      1.245        174        640: 100%|██████████| 8/8 [00:41<00:00,  5.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:15<00:00,  3.91s/it]\n",
      "                   all        128        929      0.655      0.583      0.647      0.478\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G      1.148      1.314      1.241        223        640: 100%|██████████| 8/8 [00:42<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]\n",
      "                   all        128        929      0.676       0.59      0.664      0.491\n",
      "\n",
      "3 epochs completed in 0.049 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]\n",
      "                   all        128        929      0.692       0.58      0.665      0.491\n",
      "                person        128        254      0.798      0.667      0.768       0.55\n",
      "               bicycle        128          6      0.925      0.333      0.376      0.313\n",
      "                   car        128         46      0.909      0.217      0.315      0.193\n",
      "            motorcycle        128          5      0.698        0.8      0.895      0.732\n",
      "              airplane        128          6      0.728      0.833      0.955      0.742\n",
      "                   bus        128          7      0.753      0.714      0.721      0.654\n",
      "                 train        128          3      0.674          1      0.913      0.797\n",
      "                 truck        128         12      0.921        0.5       0.53      0.358\n",
      "                  boat        128          6      0.307      0.167      0.392       0.28\n",
      "         traffic light        128         14      0.658      0.214      0.227      0.143\n",
      "             stop sign        128          2      0.781          1      0.995      0.718\n",
      "                 bench        128          9      0.806      0.556      0.622      0.485\n",
      "                  bird        128         16      0.839      0.875       0.95      0.616\n",
      "                   cat        128          4      0.714          1      0.895      0.684\n",
      "                   dog        128          9      0.557      0.778       0.79      0.548\n",
      "                 horse        128          2      0.693          1      0.995      0.484\n",
      "              elephant        128         17      0.758      0.941      0.939      0.748\n",
      "                  bear        128          1       0.49          1      0.995      0.995\n",
      "                 zebra        128          4      0.866          1      0.995      0.966\n",
      "               giraffe        128          9      0.681          1      0.973      0.718\n",
      "              backpack        128          6      0.382      0.215      0.353      0.198\n",
      "              umbrella        128         18      0.622      0.556      0.678      0.463\n",
      "               handbag        128         19          1      0.117       0.29      0.162\n",
      "                   tie        128          7      0.787      0.714      0.704      0.491\n",
      "              suitcase        128          4       0.64      0.899      0.849      0.568\n",
      "               frisbee        128          5      0.742        0.8      0.759      0.655\n",
      "                  skis        128          1       0.81          1      0.995      0.522\n",
      "             snowboard        128          7      0.726      0.857      0.865      0.518\n",
      "           sports ball        128          6      0.715       0.43      0.556      0.276\n",
      "                  kite        128         10      0.667        0.5      0.561      0.189\n",
      "          baseball bat        128          4      0.412      0.368      0.374      0.211\n",
      "        baseball glove        128          7      0.752      0.429       0.43      0.303\n",
      "            skateboard        128          5      0.759        0.6        0.6      0.407\n",
      "         tennis racket        128          7      0.519      0.313      0.545      0.326\n",
      "                bottle        128         18      0.475      0.444      0.416      0.255\n",
      "            wine glass        128         16      0.581      0.562      0.643      0.387\n",
      "                   cup        128         36      0.687      0.305      0.422      0.297\n",
      "                  fork        128          6      0.641      0.167        0.2      0.173\n",
      "                 knife        128         16      0.674      0.517      0.652      0.389\n",
      "                 spoon        128         22      0.819      0.207      0.354      0.198\n",
      "                  bowl        128         28      0.644      0.714      0.675      0.558\n",
      "                banana        128          1          0          0      0.199     0.0796\n",
      "              sandwich        128          2      0.959          1      0.995      0.995\n",
      "                orange        128          4          1          0      0.828      0.531\n",
      "              broccoli        128         11      0.586       0.26      0.293      0.238\n",
      "                carrot        128         24      0.674      0.542      0.732       0.48\n",
      "               hot dog        128          2      0.621          1      0.828      0.763\n",
      "                 pizza        128          5      0.776          1      0.995      0.841\n",
      "                 donut        128         14      0.633          1      0.952      0.865\n",
      "                  cake        128          4      0.879          1      0.995       0.89\n",
      "                 chair        128         35      0.569       0.49      0.481      0.297\n",
      "                 couch        128          6      0.557      0.429      0.708      0.517\n",
      "          potted plant        128         14      0.692      0.643      0.708      0.487\n",
      "                   bed        128          3       0.88          1      0.995      0.771\n",
      "          dining table        128         13      0.519      0.615      0.518      0.422\n",
      "                toilet        128          2          1      0.899      0.995      0.896\n",
      "                    tv        128          2       0.57        0.5      0.828      0.762\n",
      "                laptop        128          3          1          0      0.544       0.46\n",
      "                 mouse        128          2          1          0     0.0523    0.00523\n",
      "                remote        128          8      0.837        0.5      0.573      0.479\n",
      "            cell phone        128          8          0          0     0.0935     0.0471\n",
      "             microwave        128          3      0.512      0.706      0.753      0.654\n",
      "                  oven        128          5      0.457        0.4      0.485      0.353\n",
      "                  sink        128          6      0.371      0.167      0.495      0.255\n",
      "          refrigerator        128          5      0.697        0.6      0.743      0.616\n",
      "                  book        128         29      0.504       0.14      0.381       0.22\n",
      "                 clock        128          9       0.89      0.778      0.901      0.742\n",
      "                  vase        128          2       0.51          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.497      0.159\n",
      "            teddy bear        128         21      0.865       0.61      0.726      0.476\n",
      "            toothbrush        128          5          1      0.578      0.906      0.559\n",
      "Speed: 1.2ms preprocess, 74.2ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:10<00:00,  1.37s/it]\n",
      "                   all        128        929      0.728       0.58      0.668      0.492\n",
      "                person        128        254      0.799      0.657      0.768      0.551\n",
      "               bicycle        128          6      0.959      0.333      0.374      0.311\n",
      "                   car        128         46      0.912      0.217      0.316      0.193\n",
      "            motorcycle        128          5      0.661        0.8      0.895      0.725\n",
      "              airplane        128          6      0.733      0.833      0.955      0.742\n",
      "                   bus        128          7      0.769      0.714      0.721      0.654\n",
      "                 train        128          3      0.677          1      0.913      0.797\n",
      "                 truck        128         12       0.93        0.5       0.53      0.349\n",
      "                  boat        128          6       0.34      0.167      0.398      0.262\n",
      "         traffic light        128         14      0.674      0.214      0.209      0.141\n",
      "             stop sign        128          2      0.788          1      0.995      0.721\n",
      "                 bench        128          9      0.815      0.556      0.622      0.485\n",
      "                  bird        128         16      0.846      0.875      0.947      0.613\n",
      "                   cat        128          4      0.689          1      0.895      0.684\n",
      "                   dog        128          9      0.563      0.778       0.79      0.548\n",
      "                 horse        128          2        0.7          1      0.995      0.487\n",
      "              elephant        128         17       0.76      0.941       0.94      0.748\n",
      "                  bear        128          1      0.494          1      0.995      0.995\n",
      "                 zebra        128          4      0.868          1      0.995      0.959\n",
      "               giraffe        128          9      0.745          1      0.951      0.722\n",
      "              backpack        128          6      0.361      0.194      0.352      0.214\n",
      "              umbrella        128         18      0.628      0.556      0.676      0.462\n",
      "               handbag        128         19          1      0.112      0.285      0.159\n",
      "                   tie        128          7      0.791      0.714      0.703       0.49\n",
      "              suitcase        128          4      0.622      0.833      0.849      0.568\n",
      "               frisbee        128          5      0.748        0.8      0.759      0.655\n",
      "                  skis        128          1      0.822          1      0.995      0.522\n",
      "             snowboard        128          7      0.737      0.857      0.864      0.518\n",
      "           sports ball        128          6      0.712      0.424      0.556      0.276\n",
      "                  kite        128         10      0.684        0.5      0.562       0.19\n",
      "          baseball bat        128          4      0.338       0.25      0.349      0.199\n",
      "        baseball glove        128          7      0.689      0.429       0.43      0.317\n",
      "            skateboard        128          5      0.824        0.6        0.6       0.42\n",
      "         tennis racket        128          7      0.717      0.373       0.59       0.36\n",
      "                bottle        128         18      0.447      0.406      0.425      0.267\n",
      "            wine glass        128         16      0.611      0.562      0.641      0.384\n",
      "                   cup        128         36      0.666      0.333      0.443      0.307\n",
      "                  fork        128          6      0.645      0.167      0.207      0.178\n",
      "                 knife        128         16      0.581       0.52      0.625      0.371\n",
      "                 spoon        128         22        0.8      0.227      0.365      0.203\n",
      "                  bowl        128         28      0.679       0.75      0.704      0.558\n",
      "                banana        128          1          1          0      0.166     0.0746\n",
      "              sandwich        128          2          1      0.849      0.995      0.995\n",
      "                orange        128          4          1          0      0.828      0.531\n",
      "              broccoli        128         11      0.419      0.182      0.283      0.227\n",
      "                carrot        128         24      0.713      0.621       0.74      0.477\n",
      "               hot dog        128          2      0.628          1      0.828      0.796\n",
      "                 pizza        128          5      0.803          1      0.995      0.844\n",
      "                 donut        128         14      0.635          1      0.946      0.855\n",
      "                  cake        128          4      0.794          1      0.995       0.89\n",
      "                 chair        128         35      0.555      0.457      0.465      0.289\n",
      "                 couch        128          6      0.795      0.651      0.829       0.62\n",
      "          potted plant        128         14      0.698      0.643      0.708      0.487\n",
      "                   bed        128          3      0.892          1      0.995      0.679\n",
      "          dining table        128         13      0.617      0.615      0.533      0.411\n",
      "                toilet        128          2          1      0.893      0.995      0.896\n",
      "                    tv        128          2      0.575        0.5      0.828      0.763\n",
      "                laptop        128          3          1          0      0.552      0.471\n",
      "                 mouse        128          2          1          0     0.0776     0.0155\n",
      "                remote        128          8      0.842        0.5      0.578      0.483\n",
      "            cell phone        128          8          1          0     0.0935     0.0469\n",
      "             microwave        128          3      0.505      0.684      0.863      0.734\n",
      "                  oven        128          5      0.434        0.4      0.485      0.353\n",
      "                  sink        128          6      0.374      0.167      0.456      0.243\n",
      "          refrigerator        128          5      0.722        0.6      0.743      0.599\n",
      "                  book        128         29      0.571      0.138      0.396      0.228\n",
      "                 clock        128          9      0.893      0.778        0.9      0.742\n",
      "                  vase        128          2      0.514          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.497      0.159\n",
      "            teddy bear        128         21       0.86      0.588      0.722      0.472\n",
      "            toothbrush        128          5          1       0.71      0.906      0.525\n",
      "Speed: 1.2ms preprocess, 72.8ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "\n",
      "Downloading https:\\ultralytics.com\\images\\bus.jpg to bus.jpg...\n",
      "100%|██████████| 476k/476k [00:00<00:00, 970kB/s]\n",
      "image 1/1 C:\\Users\\dani\\Python\\34759_Perception\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 88.4ms\n",
      "Speed: 0.0ms preprocess, 88.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs\\detect\\train\\weights\\best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv8 requirement \"onnx>=1.12.0\" not found, attempting AutoUpdate...\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.13.1-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 8.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\dani\\anaconda3\\envs\\project\\lib\\site-packages (from onnx>=1.12.0) (4.5.0)\n",
      "Collecting protobuf<4,>=3.20.2\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "     ------------------------------------- 904.0/904.0 kB 14.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\dani\\anaconda3\\envs\\project\\lib\\site-packages (from onnx>=1.12.0) (1.24.2)\n",
      "Installing collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.13.1 protobuf-3.20.3\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  14.0s, saved as runs\\detect\\train\\weights\\best.onnx (12.2 MB)\n",
      "\n",
      "Export complete (14.2s)\n",
      "Results saved to \u001b[1mC:\\Users\\dani\\Python\\34759_Perception\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best.onnx imgsz=640 data=C:\\Users\\dani\\anaconda3\\envs\\project\\Lib\\site-packages\\ultralytics\\datasets\\coco128.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train the model using the 'coco128.yaml' dataset for 3 epochs\n",
    "results = model.train(data='coco128.yaml', epochs=3)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "results = model.val()\n",
    "\n",
    "# Export the model to ONNX format\n",
    "success = model.export(format='onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.81  Python-3.10.10 torch-2.0.0+cpu CPU\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\coco128\\labels\\train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:11<00:00,  1.39s/it]\n",
      "                   all        128        929      0.728       0.58      0.668      0.492\n",
      "                person        128        254      0.799      0.657      0.768      0.551\n",
      "               bicycle        128          6      0.959      0.333      0.374      0.311\n",
      "                   car        128         46      0.912      0.217      0.316      0.193\n",
      "            motorcycle        128          5      0.661        0.8      0.895      0.725\n",
      "              airplane        128          6      0.733      0.833      0.955      0.742\n",
      "                   bus        128          7      0.769      0.714      0.721      0.654\n",
      "                 train        128          3      0.677          1      0.913      0.797\n",
      "                 truck        128         12       0.93        0.5       0.53      0.349\n",
      "                  boat        128          6       0.34      0.167      0.398      0.262\n",
      "         traffic light        128         14      0.674      0.214      0.209      0.141\n",
      "             stop sign        128          2      0.788          1      0.995      0.721\n",
      "                 bench        128          9      0.815      0.556      0.622      0.485\n",
      "                  bird        128         16      0.846      0.875      0.947      0.613\n",
      "                   cat        128          4      0.689          1      0.895      0.684\n",
      "                   dog        128          9      0.563      0.778       0.79      0.548\n",
      "                 horse        128          2        0.7          1      0.995      0.487\n",
      "              elephant        128         17       0.76      0.941       0.94      0.748\n",
      "                  bear        128          1      0.494          1      0.995      0.995\n",
      "                 zebra        128          4      0.868          1      0.995      0.959\n",
      "               giraffe        128          9      0.745          1      0.951      0.722\n",
      "              backpack        128          6      0.361      0.194      0.352      0.214\n",
      "              umbrella        128         18      0.628      0.556      0.676      0.462\n",
      "               handbag        128         19          1      0.112      0.285      0.159\n",
      "                   tie        128          7      0.791      0.714      0.703       0.49\n",
      "              suitcase        128          4      0.622      0.833      0.849      0.568\n",
      "               frisbee        128          5      0.748        0.8      0.759      0.655\n",
      "                  skis        128          1      0.822          1      0.995      0.522\n",
      "             snowboard        128          7      0.737      0.857      0.864      0.518\n",
      "           sports ball        128          6      0.712      0.424      0.556      0.276\n",
      "                  kite        128         10      0.684        0.5      0.562       0.19\n",
      "          baseball bat        128          4      0.338       0.25      0.349      0.199\n",
      "        baseball glove        128          7      0.689      0.429       0.43      0.317\n",
      "            skateboard        128          5      0.824        0.6        0.6       0.42\n",
      "         tennis racket        128          7      0.717      0.373       0.59       0.36\n",
      "                bottle        128         18      0.447      0.406      0.425      0.267\n",
      "            wine glass        128         16      0.611      0.562      0.641      0.384\n",
      "                   cup        128         36      0.666      0.333      0.443      0.307\n",
      "                  fork        128          6      0.645      0.167      0.207      0.178\n",
      "                 knife        128         16      0.581       0.52      0.625      0.371\n",
      "                 spoon        128         22        0.8      0.227      0.365      0.203\n",
      "                  bowl        128         28      0.679       0.75      0.704      0.558\n",
      "                banana        128          1          1          0      0.166     0.0746\n",
      "              sandwich        128          2          1      0.849      0.995      0.995\n",
      "                orange        128          4          1          0      0.828      0.531\n",
      "              broccoli        128         11      0.419      0.182      0.283      0.227\n",
      "                carrot        128         24      0.713      0.621       0.74      0.477\n",
      "               hot dog        128          2      0.628          1      0.828      0.796\n",
      "                 pizza        128          5      0.803          1      0.995      0.844\n",
      "                 donut        128         14      0.635          1      0.946      0.855\n",
      "                  cake        128          4      0.794          1      0.995       0.89\n",
      "                 chair        128         35      0.555      0.457      0.465      0.289\n",
      "                 couch        128          6      0.795      0.651      0.829       0.62\n",
      "          potted plant        128         14      0.698      0.643      0.708      0.487\n",
      "                   bed        128          3      0.892          1      0.995      0.679\n",
      "          dining table        128         13      0.617      0.615      0.533      0.411\n",
      "                toilet        128          2          1      0.893      0.995      0.896\n",
      "                    tv        128          2      0.575        0.5      0.828      0.763\n",
      "                laptop        128          3          1          0      0.552      0.471\n",
      "                 mouse        128          2          1          0     0.0776     0.0155\n",
      "                remote        128          8      0.842        0.5      0.578      0.483\n",
      "            cell phone        128          8          1          0     0.0935     0.0469\n",
      "             microwave        128          3      0.505      0.684      0.863      0.734\n",
      "                  oven        128          5      0.434        0.4      0.485      0.353\n",
      "                  sink        128          6      0.374      0.167      0.456      0.243\n",
      "          refrigerator        128          5      0.722        0.6      0.743      0.599\n",
      "                  book        128         29      0.571      0.138      0.396      0.228\n",
      "                 clock        128          9      0.893      0.778        0.9      0.742\n",
      "                  vase        128          2      0.514          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.497      0.159\n",
      "            teddy bear        128         21       0.86      0.588      0.722      0.472\n",
      "            toothbrush        128          5          1       0.71      0.906      0.525\n",
      "Speed: 1.2ms preprocess, 73.1ms inference, 0.0ms loss, 2.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x0000022E5A5EB220>\n",
       "fitness: 0.5098954713297049\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.55107,     0.31111,     0.19345,     0.72492,     0.74191,     0.65397,     0.79698,     0.34852,     0.26208,     0.14073,     0.49235,       0.721,     0.49235,     0.48523,       0.613,     0.68431,     0.54845,      0.4874,     0.49235,     0.49235,     0.74843,       0.995,     0.95927,     0.72208,\n",
       "           0.21376,      0.4622,     0.15944,     0.48992,     0.56765,     0.65471,     0.52238,     0.51831,     0.27599,     0.19048,      0.1986,     0.31741,     0.41999,     0.49235,     0.36008,     0.26664,     0.38398,     0.30703,      0.1784,     0.37081,     0.20297,      0.5577,    0.074625,     0.49235,\n",
       "             0.995,     0.53075,     0.22706,     0.47694,     0.79575,     0.84392,     0.85543,     0.89015,     0.28877,     0.61966,     0.48717,     0.67911,     0.41097,     0.89619,     0.76255,     0.47128,    0.015521,     0.48266,     0.49235,     0.04686,     0.73429,     0.35298,     0.49235,     0.24349,\n",
       "           0.59878,      0.2276,     0.74157,      0.7455,     0.15858,     0.47193,     0.49235,     0.52462])\n",
       "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.7277399162948969, 'metrics/recall(B)': 0.5798276785161741, 'metrics/mAP50(B)': 0.667779534294416, 'metrics/mAP50-95(B)': 0.4923527976669592, 'fitness': 0.5098954713297049}\n",
       "save_dir: WindowsPath('runs/detect/val2')\n",
       "speed: {'preprocess': 1.2274626642465591, 'inference': 73.06399755179882, 'loss': 0.0, 'postprocess': 2.8379615396261215}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on pre-trained model from Coco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.93  Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=weights/coco.pt, data=kitty_yolov8.yaml, epochs=30, patience=50, batch=8, imgsz=(1224, 370), save=True, save_period=-1, cache=False, device=0, workers=8, project=custom_runs, name=kitty_train_coco, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=custom_runs\\kitty_train_coco\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.Detect                [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "WARNING  updating to 'imgsz=1224'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "WARNING  imgsz=[1224] must be multiple of max stride 32, updating to [1248]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\training\\labels.cache... 21155 images, 1623 backgrounds, 0 corrupt: 100%|██████████| 21155/21155 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\val\\labels.cache... 354 images, 0 backgrounds, 0 corrupt: 100%|██████████| 354/354 [00:00<?, ?it/s]\n",
      "Plotting labels to custom_runs\\kitty_train_coco\\labels.jpg... \n",
      "Image sizes 1248 train, 1248 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mcustom_runs\\kitty_train_coco\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/30      4.33G      1.108      1.306      1.055         20       1248: 100%|██████████| 2645/2645 [09:24<00:00,  4.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.08it/s]\n",
      "                   all        354       4018      0.858      0.446      0.576      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/30      4.32G      1.041     0.8971      1.037          1       1248: 100%|██████████| 2645/2645 [09:48<00:00,  4.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.63it/s]\n",
      "                   all        354       4018      0.698      0.492       0.55      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/30      4.33G      1.055     0.8831      1.048          7       1248: 100%|██████████| 2645/2645 [09:51<00:00,  4.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.03it/s]\n",
      "                   all        354       4018      0.721      0.504      0.575      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/30      4.32G      1.051      0.849       1.05         12       1248: 100%|██████████| 2645/2645 [09:13<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.21it/s]\n",
      "                   all        354       4018      0.715        0.5      0.555      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/30      4.32G      1.006     0.7868      1.033         13       1248: 100%|██████████| 2645/2645 [09:10<00:00,  4.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.12it/s]\n",
      "                   all        354       4018      0.642      0.581      0.559      0.295\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/30      4.38G     0.9723     0.7374      1.017          7       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.10it/s]\n",
      "                   all        354       4018      0.745      0.507      0.568      0.283\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/30      4.33G     0.9414     0.7032      1.002          9       1248: 100%|██████████| 2645/2645 [09:09<00:00,  4.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.86it/s]\n",
      "                   all        354       4018      0.585      0.595      0.579      0.294\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/30      4.33G     0.9255     0.6835     0.9986         14       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.05it/s]\n",
      "                   all        354       4018      0.748      0.524      0.601      0.307\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/30      4.32G     0.9011     0.6542     0.9869          6       1248: 100%|██████████| 2645/2645 [09:12<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.26it/s]\n",
      "                   all        354       4018       0.64      0.516      0.527       0.25\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/30      4.33G     0.8846     0.6378     0.9789          7       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.80it/s]\n",
      "                   all        354       4018      0.628       0.52      0.531      0.247\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/30      4.37G     0.8692     0.6217     0.9726         12       1248: 100%|██████████| 2645/2645 [09:10<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.91it/s]\n",
      "                   all        354       4018      0.593      0.515       0.54       0.25\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/30      4.33G     0.8525      0.604     0.9662         18       1248: 100%|██████████| 2645/2645 [09:15<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.78it/s]\n",
      "                   all        354       4018      0.618      0.531      0.539      0.252\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/30      4.33G     0.8414     0.5943     0.9621         21       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.27it/s]\n",
      "                   all        354       4018      0.631      0.524      0.567       0.25\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/30      4.32G     0.8302     0.5799     0.9558         14       1248: 100%|██████████| 2645/2645 [09:10<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.10it/s]\n",
      "                   all        354       4018      0.627      0.543      0.597      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/30      4.38G     0.8193     0.5706     0.9535         12       1248: 100%|██████████| 2645/2645 [09:12<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.94it/s]\n",
      "                   all        354       4018      0.588      0.585      0.586      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/30      4.32G     0.8077     0.5572     0.9458          6       1248: 100%|██████████| 2645/2645 [09:12<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.37it/s]\n",
      "                   all        354       4018      0.594      0.577      0.598      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/30      4.32G     0.8029     0.5506     0.9447         26       1248: 100%|██████████| 2645/2645 [09:13<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.17it/s]\n",
      "                   all        354       4018      0.623      0.555      0.612      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/30      4.31G     0.7892     0.5393     0.9377         17       1248: 100%|██████████| 2645/2645 [09:13<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.98it/s]\n",
      "                   all        354       4018      0.566      0.593      0.608      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/30      4.33G     0.7758     0.5258     0.9328          7       1248: 100%|██████████| 2645/2645 [09:12<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.05it/s]\n",
      "                   all        354       4018       0.65      0.575      0.614      0.239\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/30      4.37G       0.77     0.5193     0.9319         20       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.06it/s]\n",
      "                   all        354       4018      0.623      0.604      0.621      0.252\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/30      4.32G     0.7642     0.5115     0.9292         20       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.20it/s]\n",
      "                   all        354       4018      0.652      0.592       0.63      0.272\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/30      4.33G     0.7487     0.5007     0.9235          7       1248: 100%|██████████| 2645/2645 [09:10<00:00,  4.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.30it/s]\n",
      "                   all        354       4018      0.644        0.6      0.632      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/30      4.28G     0.7401     0.4934      0.922          5       1248: 100%|██████████| 2645/2645 [09:10<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.82it/s]\n",
      "                   all        354       4018      0.657      0.603      0.645      0.278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/30      4.32G      0.727     0.4799     0.9138          7       1248: 100%|██████████| 2645/2645 [09:14<00:00,  4.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.03it/s]\n",
      "                   all        354       4018      0.659      0.594      0.652      0.278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/30      4.32G     0.7198     0.4743     0.9117          7       1248: 100%|██████████| 2645/2645 [09:16<00:00,  4.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.00it/s]\n",
      "                   all        354       4018      0.666      0.594      0.641      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/30      4.33G     0.7101     0.4667     0.9088         14       1248: 100%|██████████| 2645/2645 [09:12<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.34it/s]\n",
      "                   all        354       4018      0.675      0.591      0.654      0.265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/30      4.37G     0.7057     0.4613     0.9063          7       1248: 100%|██████████| 2645/2645 [09:12<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  4.96it/s]\n",
      "                   all        354       4018      0.709      0.591       0.66      0.271\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/30      4.29G     0.6939     0.4533     0.9026         17       1248: 100%|██████████| 2645/2645 [09:11<00:00,  4.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.07it/s]\n",
      "                   all        354       4018       0.65      0.578      0.646      0.272\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/30      4.37G     0.6859     0.4439     0.8988          6       1248: 100%|██████████| 2645/2645 [09:13<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.35it/s]\n",
      "                   all        354       4018      0.666      0.563      0.645      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/30      4.32G     0.6715     0.4337     0.8968         17       1248: 100%|██████████| 2645/2645 [09:15<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:06<00:00,  3.63it/s]\n",
      "                   all        354       4018      0.663      0.575      0.652      0.263\n",
      "\n",
      "30 epochs completed in 4.669 hours.\n",
      "Optimizer stripped from custom_runs\\kitty_train_coco\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from custom_runs\\kitty_train_coco\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating custom_runs\\kitty_train_coco\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.93  Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:05<00:00,  4.14it/s]\n",
      "                   all        354       4018      0.721      0.503      0.575      0.311\n",
      "            Pedestrian        354       2809      0.759      0.581      0.667      0.332\n",
      "               Cyclist        354        373      0.642      0.357      0.379      0.203\n",
      "                   Car        354        836      0.764      0.572      0.679      0.398\n",
      "Speed: 0.4ms preprocess, 2.5ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mcustom_runs\\kitty_train_coco\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('model/weights/coco.pt')\n",
    "\n",
    "results = model.train(data='model/kitty_yolov8.yaml',\n",
    "                      imgsz=(1224, 370),\n",
    "                      epochs=30,\n",
    "                      batch=8,\n",
    "                      project='custom_runs',\n",
    "                      name='kitty_train_coco',\n",
    "                    #   optimizer='SGD',\n",
    "                      device=0,\n",
    "                      seed=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Coco model on custom validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.93  Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dani\\Python\\34759_Perception\\datasets\\custom\\val\\labels.cache... 354 images, 0 backgrounds, 0 corrupt: 100%|██████████| 354/354 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:04<00:00,  5.46it/s]\n",
      "                   all        354       4018       0.42      0.251       0.27      0.105\n",
      "                person        354       2809      0.795      0.574      0.617      0.276\n",
      "               bicycle        354        373     0.0177    0.00536    0.00328   0.000796\n",
      "                   car        354        836      0.447      0.173       0.19     0.0373\n",
      "Speed: 0.2ms preprocess, 3.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x0000025F03363550>\n",
       "fitness: 0.12120806151964417\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.27596,  0.00079621,    0.037281,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,\n",
       "           0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,\n",
       "           0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,\n",
       "           0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468,     0.10468])\n",
       "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.4201806217840893, 'metrics/recall(B)': 0.25101086982288295, 'metrics/mAP50(B)': 0.2699559560229899, 'metrics/mAP50-95(B)': 0.10468051768593908, 'fitness': 0.12120806151964417}\n",
       "save_dir: WindowsPath('runs/detect/val3')\n",
       "speed: {'preprocess': 0.23907254644706424, 'inference': 3.4632575040483204, 'loss': 0.0, 'postprocess': 1.3479701543258409}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('model/weights/coco.pt')\n",
    "model.val(data='model/kitty_yolov8.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
